{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177a4214",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "177a4214",
        "outputId": "5ecd4a31-8722-40fc-a16a-242e3a8c4ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MathGPT Code For Final Assignment\n"
          ]
        }
      ],
      "source": [
        "print(\"MathGPT Code For Final Assignment\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# random.seed(1337)\n",
        "\n",
        "# # STAGE 1 — SIMPLE SINGLE-DIGIT OPERATIONS\n",
        "# # ----------------------------------------------\n",
        "\n",
        "# def simple_addition(n=10000):\n",
        "#     print(\"Single-digit addition (10,000)...\")\n",
        "#     problems = []\n",
        "#     for _ in range(n):\n",
        "#         a = random.randint(0, 9)\n",
        "#         b = random.randint(0, 9)\n",
        "#         problems.append(f\"{a}+{b}={a+b}\")\n",
        "#     return problems\n",
        "\n",
        "\n",
        "# def simple_subtraction(n=10000):\n",
        "#     print(\"Single-digit subtraction (10,000)...\")\n",
        "#     problems = []\n",
        "#     while len(problems) < n:\n",
        "#         a = random.randint(0, 9)\n",
        "#         b = random.randint(0, 9)\n",
        "#         if a >= b:\n",
        "#             problems.append(f\"{a}-{b}={a-b}\")\n",
        "#     return problems\n",
        "\n",
        "\n",
        "# def simple_multiplication(n=10000):\n",
        "#     print(\"Single-digit multiplication (10,000)...\")\n",
        "#     problems = []\n",
        "#     for _ in range(n):\n",
        "#         a = random.randint(0, 9)\n",
        "#         b = random.randint(0, 9)\n",
        "#         problems.append(f\"{a}*{b}={a*b}\")\n",
        "#     return problems\n",
        "\n",
        "\n",
        "# def simple_division(n=10000):\n",
        "#     print(\"Single-digit division (10,000)...\")\n",
        "#     problems = []\n",
        "#     while len(problems) < n:\n",
        "#         b = random.randint(1, 9)\n",
        "#         quotient = random.randint(0, 9)\n",
        "#         a = b * quotient\n",
        "#         if a <= 81:\n",
        "#             problems.append(f\"{a}/{b}={quotient}\")\n",
        "#     return problems\n",
        "\n",
        "# def generate_stage1(n_each=10000):\n",
        "#     print(\"Generating STAGE 1...\")\n",
        "#     return (\n",
        "#         simple_addition(n_each)\n",
        "#         + simple_subtraction(n_each)\n",
        "#         + simple_multiplication(n_each)\n",
        "#         + simple_division(n_each)\n",
        "#     )\n",
        "\n",
        "# # STAGE 2 — DOUBLE-DIGIT OPERATIONS\n",
        "# # ----------------------------------------------\n",
        "# def double_addition(n=10000):\n",
        "#     print(\"Two-digit addition (10,000)...\")\n",
        "#     problems = []\n",
        "#     for _ in range(n):\n",
        "#         a = random.randint(10, 99)\n",
        "#         b = random.randint(10, 99)\n",
        "#         problems.append(f\"{a}+{b}={a+b}\")\n",
        "#     return problems\n",
        "\n",
        "\n",
        "# def double_subtraction(n=10000):\n",
        "#     print(\"Two-digit subtraction (10,000)...\")\n",
        "#     problems = []\n",
        "#     for _ in range(n):\n",
        "#         a = random.randint(10, 99)\n",
        "#         b = random.randint(10, 99)\n",
        "#         if a >= b:\n",
        "#             problems.append(f\"{a}-{b}={a-b}\")\n",
        "#         else:\n",
        "#             problems.append(f\"{b}-{a}={b-a}\")\n",
        "#     return problems\n",
        "\n",
        "# def double_multiplication(n=10000):\n",
        "#     print(\"Two-digit multiplication (10,000)...\")\n",
        "#     problems = []\n",
        "\n",
        "#     while len(problems) < n:\n",
        "#         a = random.randint(10, 99)\n",
        "#         b = random.randint(10, 99)\n",
        "#         product = a * b\n",
        "\n",
        "#         if product < 1000:  # ensure NOT a 4-digit result\n",
        "#             problems.append(f\"{a}*{b}={product}\")\n",
        "\n",
        "#     return problems\n",
        "\n",
        "# def double_division(n=10000):\n",
        "#     print(\"Two-digit division (10,000) ...\")\n",
        "#     problems = []\n",
        "\n",
        "#     while len(problems) < n:\n",
        "#         b = random.randint(10, 99)\n",
        "#         q = random.randint(1, 9)\n",
        "#         a = b * q\n",
        "\n",
        "#         if 10 <= a <= 99:                 # ensure dividend is two-digit\n",
        "#             problems.append(f\"{a}/{b}={q}\")\n",
        "\n",
        "#     return problems\n",
        "\n",
        "# def generate_stage2(n_each=10000):\n",
        "#     print(\"Generating STAGE 2...\")\n",
        "#     return (\n",
        "#         double_addition(n_each)\n",
        "#         + double_subtraction(n_each)\n",
        "#         + double_multiplication(n_each)\n",
        "#         + double_division(n_each)\n",
        "#     )\n",
        "\n",
        "# # STAGE 3 — PARENTHESIS OPERATIONS\n",
        "# # ----------------------------------------------\n",
        "\n",
        "\n",
        "# def multiplication_parentheses(n=10000):\n",
        "#     print(\"Parentheses with multiply (10,000)...\")\n",
        "#     problems = []\n",
        "#     while len(problems) < n:\n",
        "#         a = random.randint(0, 9)\n",
        "#         b = random.randint(0, 9)\n",
        "#         c = random.randint(2, 9)\n",
        "\n",
        "#         if random.random() < 0.5:\n",
        "#             problems.append(f\"({a}+{b})*{c}={(a+b)*c}\")\n",
        "#         else:\n",
        "#             if a >= b:\n",
        "#                 problems.append(f\"({a}-{b})*{c}={(a-b)*c}\")\n",
        "#     return problems\n",
        "\n",
        "\n",
        "# def division_parentheses(n=10000):\n",
        "#     print(\"Parentheses with divide (10,000)...\")\n",
        "#     problems = []\n",
        "#     while len(problems) < n:\n",
        "#         c = random.randint(2, 9)\n",
        "#         quotient = random.randint(1, 9)\n",
        "#         inner = c * quotient\n",
        "\n",
        "#         if random.random() < 0.5:\n",
        "#             # (a+b)/c\n",
        "#             a = random.randint(0, min(9, inner))\n",
        "#             b = inner - a\n",
        "#             if b <= 9:\n",
        "#                 problems.append(f\"({a}+{b})/{c}={quotient}\")\n",
        "#         else:\n",
        "#             # (a-b)/c\n",
        "#             b = random.randint(0, 9)\n",
        "#             a = inner + b\n",
        "#             if a <= 18:\n",
        "#                 problems.append(f\"({a}-{b})/{c}={quotient}\")\n",
        "#     return problems\n",
        "\n",
        "# def generate_stage3(n_each=10000):\n",
        "#     print(\"Generating STAGE 3...\")\n",
        "#     return (\n",
        "#         multiplication_parentheses(n_each)\n",
        "#         + division_parentheses(n_each)\n",
        "#     )\n",
        "\n",
        "# # Save Datasets | Split Datasets\n",
        "# # ----------------------------------------------\n",
        "\n",
        "# def save_dataset(dataset, filename):\n",
        "#     with open(filename, \"w\") as f:\n",
        "#         for problem in dataset:\n",
        "#             f.write(problem + \"\\n\")\n",
        "#     print(f\"Saved {len(dataset)} problems to {filename}\")\n",
        "\n",
        "# def split_train_test(data, test_ratio=0.1):\n",
        "#     data = data.copy()\n",
        "#     random.shuffle(data)\n",
        "#     split = int((1 - test_ratio) * len(data))\n",
        "#     return data[:split], data[split:]\n",
        "\n",
        "# # GENERATE Datasets for all Stages\n",
        "# # ----------------------------------------------\n",
        "\n",
        "# stage1_data = generate_stage1()\n",
        "# stage1_train, stage1_test = split_train_test(stage1_data)\n",
        "# save_dataset(stage1_train,\"stage1_data.txt\")\n",
        "\n",
        "\n",
        "# stage2_data = generate_stage2()\n",
        "# stage2_train, stage2_test = split_train_test(stage2_data)\n",
        "# save_dataset((stage1_train + stage2_train),\"stage2_data.txt\")\n",
        "\n",
        "\n",
        "# stage3_data = generate_stage3()\n",
        "# stage3_train, stage3_test = split_train_test(stage3_data)\n",
        "# save_dataset((stage1_train + stage2_train + stage3_train),\"stage3_data.txt\")\n",
        "\n",
        "\n",
        "# # Final held-out dataset for test\n",
        "# # ----------------------------------------------\n",
        "# final_test_data = stage1_test + stage2_test + stage3_test\n",
        "# random.shuffle(final_test_data)\n",
        "# save_dataset(final_test_data,\"final_test_data.txt\")\n",
        "\n",
        "\n",
        "def describe_dataset(path, num_samples=3):\n",
        "    # read file\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # stats\n",
        "    total_chars = len(text)\n",
        "    total_lines = text.count(\"\\n\") + 1\n",
        "    vocab = sorted(list(set(text)))\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # sample lines (non-empty)\n",
        "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "    sample_lines = lines[:num_samples]\n",
        "\n",
        "    print(\"=== Dataset Description ===\")\n",
        "    print(f\"File: {path}\")\n",
        "    print(f\"Total characters: {total_chars}\")\n",
        "    print(f\"Total lines: {total_lines}\")\n",
        "    print(\"Sample lines:\")\n",
        "    for i, s in enumerate(sample_lines):\n",
        "        print(f\"{i+1}. {s}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "describe_dataset(\"final_test_data.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-IaVPHDqvFU",
        "outputId": "287e342f-cecd-4e5f-e8cb-d7db3d14dd16"
      },
      "id": "v-IaVPHDqvFU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dataset Description ===\n",
            "File: final_test_data.txt\n",
            "Total characters: 83435\n",
            "Total lines: 10001\n",
            "Vocabulary size: 18\n",
            "Vocabulary: ['\\n', '(', ')', '*', '+', '-', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=']\n",
            "Sample lines:\n",
            "1. 5*9=45\n",
            "2. 47-34=13\n",
            "3. (12-9)/3=1\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import random\n",
        "random.seed(1337)\n",
        "\n",
        "#hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 2000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 192\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.1\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Vocabulary\n",
        "# -------------------------------------------------\n",
        "VOCAB = [\n",
        "    \"PAD\", \"ANS\", \"STOP\",\n",
        "    \"LPAREN\", \"RPAREN\",\n",
        "    \"PLUS\", \"MINUS\", \"MUL\", \"DIV\",\n",
        "    \"NUM_0\",\"NUM_1\",\"NUM_2\",\"NUM_3\",\"NUM_4\",\n",
        "    \"NUM_5\",\"NUM_6\",\"NUM_7\",\"NUM_8\",\"NUM_9\"\n",
        "]\n",
        "vocab_size = len(VOCAB)\n",
        "\n",
        "# create a mapping from tokens to integers\n",
        "stoi = {tok:i for i,tok in enumerate(VOCAB)}\n",
        "itos = {i:t for t,i in stoi.items()}\n",
        "\n",
        "# encoder: take a math problem, output a list of token\n",
        "def encode_expression(expr):\n",
        "    lhs, rhs = expr.split(\"=\")\n",
        "    tokens = []\n",
        "\n",
        "    for ch in lhs:\n",
        "        if ch.isdigit():\n",
        "            tokens.append(f\"NUM_{ch}\")\n",
        "        elif ch == \"+\":\n",
        "            tokens.append(\"PLUS\")\n",
        "        elif ch == \"-\":\n",
        "            tokens.append(\"MINUS\")\n",
        "        elif ch == \"*\":\n",
        "            tokens.append(\"MUL\")\n",
        "        elif ch == \"/\":\n",
        "            tokens.append(\"DIV\")\n",
        "        elif ch == \"(\":\n",
        "            tokens.append(\"LPAREN\")\n",
        "        elif ch == \")\":\n",
        "            tokens.append(\"RPAREN\")\n",
        "\n",
        "    tokens.append(\"ANS\")\n",
        "\n",
        "    for ch in rhs:\n",
        "        tokens.append(f\"NUM_{ch}\")\n",
        "\n",
        "    tokens.append(\"STOP\")\n",
        "\n",
        "    ids = [stoi[t] for t in tokens]\n",
        "\n",
        "    ids += [stoi[\"PAD\"]] * (block_size - len(ids))\n",
        "    return ids\n",
        "\n",
        "# decoder: take a list of integers, outputs tokens\n",
        "def decode_tokens(token_ids):\n",
        "    return [itos[i] for i in token_ids]\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data), (batch_size,))\n",
        "    x = torch.stack([data[i, :-1] for i in ix])\n",
        "    y = torch.stack([data[i, 1:] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# # Masked loss (answer-only)\n",
        "# def masked_loss(logits, targets):\n",
        "#     B, T, C = logits.shape\n",
        "\n",
        "#     logits = logits.view(B * T, C)\n",
        "#     targets = targets.view(B * T)\n",
        "\n",
        "#     ans_id = stoi[\"ANS\"]\n",
        "#     stop_id = stoi[\"STOP\"]\n",
        "\n",
        "#     mask = torch.zeros_like(targets, dtype=torch.bool)\n",
        "#     targets_2d = targets.view(B, T)\n",
        "\n",
        "#     for i in range(B):\n",
        "#         row = targets_2d[i]\n",
        "\n",
        "#         ans_pos = (row == ans_id).nonzero(as_tuple=True)[0]\n",
        "#         stop_pos = (row == stop_id).nonzero(as_tuple=True)[0]\n",
        "\n",
        "#         if len(ans_pos) > 0 and len(stop_pos) > 0:\n",
        "#             start = ans_pos[0] + 1      # first answer token\n",
        "#             end = stop_pos[0] + 1       # include STOP\n",
        "#             mask[i*T + start : i*T + end] = True\n",
        "\n",
        "#     return F.cross_entropy(logits[mask], targets[mask])\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets,ignore_index=stoi[\"PAD\"])\n",
        "            #loss = masked_loss(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "\n",
        "            # not using softmax\n",
        "            # probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # apply argmax to choose highest probability token\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "            if idx_next.item() == stoi[\"STOP\"]:\n",
        "                break\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_stage(model, optimizer,data, max_iters, stage_name):\n",
        "    print(f\"\\n================ {stage_name} =================\")\n",
        "    global train_data, val_data\n",
        "\n",
        "    perm = torch.randperm(len(data))\n",
        "    data = data[perm]\n",
        "\n",
        "    split = int(0.9 * len(data))\n",
        "    train_data = data[:split]\n",
        "    val_data = data[split:]\n",
        "\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss(model)\n",
        "            # acc = eval_accuracy(model,val_data[:300])\n",
        "            print(\n",
        "                f\"step {iter}: \"\n",
        "                f\"train loss {losses['train']:.4f}, \"\n",
        "                f\"val loss {losses['val']:.4f}, \"\n",
        "                # f\"accuracy {acc*100:.1f}%\\n\"\n",
        "            )\n",
        "\n",
        "        xb, yb = get_batch(\"train\")\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "# Dataset loading\n",
        "# -------------------------------------------------\n",
        "def load_dataset(path):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            encoded = encode_expression(line.strip())\n",
        "            if encoded is not None:\n",
        "                data.append(encoded)\n",
        "    return torch.tensor(data, dtype=torch.long)\n",
        "\n",
        "\n",
        "stage1_data = load_dataset(\"stage1_data.txt\")\n",
        "stage2_data = load_dataset(\"stage2_data.txt\")\n",
        "stage3_data = load_dataset(\"stage3_data.txt\")\n",
        "\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "#print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# train_stage(\n",
        "#     m,\n",
        "#     optimizer,\n",
        "#     stage1_data,\n",
        "#     max_iters=2000,\n",
        "#     stage_name=\"STAGE 1: SIMPLE OPS\"\n",
        "# )\n",
        "# torch.save(m.state_dict(), \"model_weights_part1.pt\")\n",
        "# print(\"Saved model_weights_part1.pt\")\n",
        "\n",
        "\n",
        "# print(\"\\nLoading Stage-1 weights for Stage-2 training...\")\n",
        "# m.load_state_dict(torch.load(\"model_weights_part1.pt\"))\n",
        "# optimizer = torch.optim.AdamW(m.parameters(), lr=3e-4)\n",
        "# train_stage(\n",
        "#     m,\n",
        "#     optimizer,\n",
        "#     stage2_data,\n",
        "#     max_iters=2000,\n",
        "#     stage_name=\"STAGE 2: DOUBLE DIGIT OPS\"\n",
        "# )\n",
        "# torch.save(m.state_dict(), \"model_weights_part2.pt\")\n",
        "# print(\"Saved model_weights_part2.pt\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nLoading Stage-2 weights for Stage-3 training...\")\n",
        "m.load_state_dict(torch.load(\"model_weights_part2.pt\"))\n",
        "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-4)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "train_stage(\n",
        "    m,\n",
        "    optimizer,\n",
        "    stage3_data,\n",
        "    max_iters=5000,\n",
        "    stage_name=\"STAGE 3: FULL DATASET\"\n",
        ")\n",
        "torch.save(m.state_dict(), \"Model_weights_part3.pt\")\n",
        "print(\"Saved Model_weights_part3.pt\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewu1PLPF1qWN",
        "outputId": "ac2986f3-b4b2-4a12-ae7e-a55609d80200"
      },
      "id": "ewu1PLPF1qWN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.679571 M parameters\n",
            "\n",
            "Loading Stage-2 weights for Stage-3 training...\n",
            "\n",
            "================ STAGE 3: FULL DATASET =================\n",
            "step 0: train loss 2.3777, val loss 2.4210, \n",
            "step 500: train loss 0.9837, val loss 0.9874, \n",
            "step 1000: train loss 0.9571, val loss 0.9584, \n",
            "step 1500: train loss 0.9307, val loss 0.9363, \n",
            "step 2000: train loss 0.9155, val loss 0.9198, \n",
            "step 2500: train loss 0.9103, val loss 0.9143, \n",
            "step 3000: train loss 0.8956, val loss 0.8986, \n",
            "step 3500: train loss 0.8927, val loss 0.8999, \n",
            "step 4000: train loss 0.8889, val loss 0.8870, \n",
            "step 4500: train loss 0.8911, val loss 0.8933, \n",
            "step 4999: train loss 0.8790, val loss 0.8840, \n",
            "Saved Model_weights_part3.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing generated model on unseen Held-out Dataset\n",
        "# ----------------------------------------------\n",
        "\n",
        "def classify_category(tokens):\n",
        "    # parentheses take precedence\n",
        "    if \"LPAREN\" in tokens or \"RPAREN\" in tokens:\n",
        "        return \"paren\"\n",
        "\n",
        "    # look only before ANS\n",
        "    for t in tokens:\n",
        "        if t == \"ANS\":\n",
        "            break\n",
        "        if t == \"PLUS\":\n",
        "            return \"add\"\n",
        "        if t == \"MINUS\":\n",
        "            return \"sub\"\n",
        "        if t == \"MUL\":\n",
        "            return \"mul\"\n",
        "        if t == \"DIV\":\n",
        "            return \"div\"\n",
        "\n",
        "    return \"unknown\"\n",
        "\n",
        "\n",
        "\n",
        "def tokens_to_number(tokens):\n",
        "    digits = []\n",
        "    after_ans = False\n",
        "    for t in tokens:\n",
        "        if t == \"ANS\":\n",
        "            after_ans = True\n",
        "        elif after_ans:\n",
        "            if t.startswith(\"NUM_\"):\n",
        "                digits.append(t[-1])\n",
        "            elif t == \"STOP\":\n",
        "                break\n",
        "    return \"\".join(digits)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_accuracy(model, test_data):\n",
        "    model.eval()\n",
        "\n",
        "    stats = {\n",
        "        \"add\":   {\"correct\": 0, \"total\": 0},\n",
        "        \"sub\":   {\"correct\": 0, \"total\": 0},\n",
        "        \"mul\":   {\"correct\": 0, \"total\": 0},\n",
        "        \"div\":   {\"correct\": 0, \"total\": 0},\n",
        "        \"paren\": {\"correct\": 0, \"total\": 0},\n",
        "    }\n",
        "\n",
        "    for row in test_data:\n",
        "        row = row.unsqueeze(0).to(device)\n",
        "\n",
        "        decoded = decode_tokens(row[0].tolist())\n",
        "        category = classify_category(decoded)\n",
        "\n",
        "        # build prompt up to ANS\n",
        "        ans_pos = (row[0] == stoi[\"ANS\"]).nonzero(as_tuple=True)[0][0]\n",
        "        prompt = row[:, :ans_pos + 1]\n",
        "\n",
        "        out = model.generate(prompt, max_new_tokens=block_size)\n",
        "        pred = tokens_to_number(decode_tokens(out[0].tolist()))\n",
        "        gold = tokens_to_number(decoded)\n",
        "\n",
        "        if category in stats:\n",
        "            stats[category][\"total\"] += 1\n",
        "            if pred == gold:\n",
        "                stats[category][\"correct\"] += 1\n",
        "\n",
        "    print(\"\\nCategory-wise accuracy:\")\n",
        "    overall_correct = 0\n",
        "    overall_total = 0\n",
        "\n",
        "    for k in [\"add\", \"sub\", \"mul\", \"div\", \"paren\"]:\n",
        "        c = stats[k][\"correct\"]\n",
        "        t = stats[k][\"total\"]\n",
        "        acc = 100 * c / t if t > 0 else 0.0\n",
        "        print(f\"{k:>7}: {acc:5.2f}% ({c}/{t})\")\n",
        "        overall_correct += c\n",
        "        overall_total += t\n",
        "\n",
        "    print(f\"\\nOverall: {100*overall_correct/overall_total:.2f}% \"\n",
        "          f\"({overall_correct}/{overall_total})\")\n",
        "\n",
        "    return overall_correct / overall_total\n",
        "\n",
        "\n",
        "\n",
        "model = GPTLanguageModel().to(device)\n",
        "model.load_state_dict(torch.load(\"Model_weights_part3.pt\"))\n",
        "\n",
        "test_data = load_dataset(\"final_test_data.txt\")\n",
        "eval_accuracy(model, test_data)\n",
        "\n",
        "\n",
        "# Testing random guess baseline on unseen Held-out Dataset\n",
        "# ----------------------------------------------\n",
        "\n",
        "def eval_random_baseline(test_data):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for row in test_data:\n",
        "        tokens = decode_tokens(row.tolist())\n",
        "        gold = tokens_to_number(tokens)\n",
        "\n",
        "        # random number with same digit length\n",
        "        rand_len = len(gold)\n",
        "        pred = \"\".join(str(random.randint(0,9)) for _ in range(rand_len))\n",
        "\n",
        "        if pred == gold:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Random baseline accuracy: {acc*100:.2f}% ({correct}/{total})\")\n",
        "    return acc\n",
        "\n",
        "eval_random_baseline(test_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "v9_qjRqopDMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42181276-4e08-4198-9034-e835f9605f5a"
      },
      "id": "v9_qjRqopDMf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Category-wise accuracy:\n",
            "    add: 98.80% (1980/2004)\n",
            "    sub: 93.44% (1866/1997)\n",
            "    mul: 71.58% (1456/2034)\n",
            "    div: 100.00% (1965/1965)\n",
            "  paren: 83.15% (1663/2000)\n",
            "\n",
            "Overall: 89.30% (8930/10000)\n",
            "Random baseline accuracy: 5.74% (574/10000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0574"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_expression(tokens):\n",
        "    expr = []\n",
        "\n",
        "    for t in tokens:\n",
        "        if t == \"ANS\":\n",
        "            expr.append(\"=\")\n",
        "        elif t == \"STOP\":\n",
        "            break\n",
        "        elif t.startswith(\"NUM_\"):\n",
        "            expr.append(t[-1])\n",
        "        elif t == \"PLUS\":\n",
        "            expr.append(\"+\")\n",
        "        elif t == \"MINUS\":\n",
        "            expr.append(\"-\")\n",
        "        elif t == \"MUL\":\n",
        "            expr.append(\"*\")\n",
        "        elif t == \"DIV\":\n",
        "            expr.append(\"/\")\n",
        "        elif t == \"LPAREN\":\n",
        "            expr.append(\"(\")\n",
        "        elif t == \"RPAREN\":\n",
        "            expr.append(\")\")\n",
        "        elif t == \"PAD\":\n",
        "            continue\n",
        "\n",
        "    return \"\".join(expr)\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "@torch.no_grad()\n",
        "def generat_samples(model, test_data, n_examples=10):\n",
        "    model.eval()\n",
        "\n",
        "    indices = random.sample(range(len(test_data)), n_examples)\n",
        "\n",
        "    print(\"\\nRandom model outputs:\")\n",
        "\n",
        "    for i, idx in enumerate(indices, 1):\n",
        "        row = test_data[idx].unsqueeze(0).to(device)\n",
        "\n",
        "        # prompt up to ANS\n",
        "        ans_pos = (row[0] == stoi[\"ANS\"]).nonzero(as_tuple=True)[0][0]\n",
        "        prompt = row[:, :ans_pos + 1]\n",
        "\n",
        "        out = model.generate(prompt, max_new_tokens=block_size)\n",
        "        tokens = decode_tokens(out[0].tolist())\n",
        "\n",
        "        expr = tokens_to_expression(tokens)\n",
        "\n",
        "        print(f\"{i:02d}: {expr}\")\n",
        "\n",
        "\n",
        "generat_samples(\n",
        "    model,\n",
        "    test_data,\n",
        "    n_examples=10\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UDxT2NfbX-W",
        "outputId": "88960118-5ce1-4319-c530-02e9849987bf"
      },
      "id": "0UDxT2NfbX-W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random model outputs:\n",
            "01: (16-4)/6=2\n",
            "02: 0+4=4\n",
            "03: 9-3=6\n",
            "04: 44/44=1\n",
            "05: 4*4=16\n",
            "06: 10*74=740\n",
            "07: 45*12=540\n",
            "08: 99+43=142\n",
            "09: 37-37=0\n",
            "10: 8*0=0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}